{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from utils import test, train_keep_best \n",
    "from mydatasets import get_dataset\n",
    "from mymodels import MLP, CNN, CNN_cifar\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f718ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main python function\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # parse the command line arguments\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    \n",
    "    # argument for training only\n",
    "    parser.add_argument('--train', action='store_true', default=False,\n",
    "                        help='train the model')\n",
    "    \n",
    "    # argument for testing only\n",
    "    parser.add_argument('--test', action='store_true', default=False,\n",
    "                        help='test the model')\n",
    "    \n",
    "    # argument to print the model\n",
    "    parser.add_argument('--print', action='store_true', default=False,\n",
    "                        help='print the model')\n",
    "    \n",
    "    # argument to print the model and trainable parameters\n",
    "    parser.add_argument('--info', action='store_true', default=False,\n",
    "                        help='print the model info')\n",
    "    \n",
    "    # argument to name the trained model\n",
    "    parser.add_argument('--name', type=str, default='mnist.pt',\n",
    "                        help='name of the trained model')\n",
    "\n",
    "    # argument to decide the dataset\n",
    "    parser.add_argument('--dataset', type=str, default='mnist',\n",
    "                        help='dataset to use: mnist or fashion_mnist')\n",
    "    \n",
    "    # argument to decide the model type\n",
    "    parser.add_argument('--model', type=str, default='mlp',\n",
    "                        help='model to use: mlp or cnn')\n",
    "    \n",
    "    # argument to decide the batch size\n",
    "    parser.add_argument('--batch_size', type=int, default=64,\n",
    "                        help='batch size for training and testing')\n",
    "    \n",
    "    # argument to decide the learning rate\n",
    "    parser.add_argument('--lr', type=float, default=0.01,\n",
    "                        help='learning rate for training')\n",
    "    \n",
    "    # argument to resume training\n",
    "    parser.add_argument('--resume', action='store_true', default=False,\n",
    "                        help='learning rate for training')\n",
    "    \n",
    "    # argument to get image input from command line to classify\n",
    "    parser.add_argument('--image', type=str, default=None,\n",
    "                        help='image file to classify')\n",
    "    \n",
    "    # argument to decide the number of convolutional layers\n",
    "    parser.add_argument('--conv_layers', type=int, default=3,\n",
    "                        help='number of convolutional layers')\n",
    "    \n",
    "    # argument to decide the number of filers in each convolutional layer\n",
    "    parser.add_argument('--filters', type=int, default=32,\n",
    "                        help='number of filers in each convolutional layer')\n",
    "    \n",
    "    # argument to decide the number of hidden layers\n",
    "    parser.add_argument('--hidden_layers', type=int, default=1,\n",
    "                        help='number of hidden layers')\n",
    "    \n",
    "    # argument to decide the number of neuron in each hidden layer\n",
    "    parser.add_argument('--neurons', type=int, default=128,\n",
    "                        help='number of neuron in each hidden layer')\n",
    "    \n",
    "    # argument to decide the number of neurons in the last fully connected layer\n",
    "    parser.add_argument('--neurons_last', type=int, default=128,\n",
    "                        help='number of neurons in the last fully connected layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11104f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #     args = parser.parse_args()\n",
    "    # This is a notebook. So you have to put the arguments hard-coded      \n",
    "    args = parser.parse_args(\"--info --dataset cifar10 --model cnn_cifar --conv_layers 5 --filters 64 --hidden_layers 3 --neurons 1024 --neurons_last 2048 --name custom_cnn.pt\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # decide which dataset to use\n",
    "    if args.dataset == 'mnist':\n",
    "        input_size = 28\n",
    "        channels = 1\n",
    "        output_size = 10\n",
    "    elif args.dataset == 'fashion_mnist':\n",
    "        input_size = 28\n",
    "        channels = 1\n",
    "        output_size = 10\n",
    "    elif args.dataset == 'cifar10':\n",
    "        input_size = 32\n",
    "        channels = 3\n",
    "        output_size = 10\n",
    "    elif args.dataset == 'cifar100':\n",
    "        input_size = 32\n",
    "        channels = 3\n",
    "        output_size = 100\n",
    "    else:\n",
    "        print('Invalid dataset')\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # define the model and configure the number of layers\n",
    "    if args.model == 'mlp':\n",
    "        hidden_size = 512\n",
    "        model = MLP(input_size, channels, hidden_size, output_size).to(device)\n",
    "    elif args.model == 'cnn':\n",
    "        if args.dataset == 'mnist' or args.dataset == 'fashion_mnist':\n",
    "            model = CNN_mnist(input_size, channels, output_size).to(device)\n",
    "        elif args.dataset == 'cifar10' or args.dataset == 'cifar100':\n",
    "            model = CNN(input_size, channels, output_size).to(device)\n",
    "    elif args.model == 'cnn_cifar':\n",
    "        if args.dataset == 'cifar10' or args.dataset == 'cifar100':\n",
    "            model = CNN_cifar(input_size, channels, output_size, args.conv_layers, args.filters, args.hidden_layers, args.neurons, args.neurons_last).to(device)\n",
    "    else:\n",
    "        print('Invalid model')\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f03ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # train the model\n",
    "    if args.train:\n",
    "        train_loader, test_loader = get_dataset(args.dataset, args.batch_size) \n",
    "        acc_now = 0.0\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            acc_now = train_keep_best(epoch, model, args.model, device, train_loader, test_loader, optimizer, criterion, input_size, channels, args.name, acc_now)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbb610",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if args.test:\n",
    "        train_loader, test_loader = get_dataset(args.dataset, args.batch_size) \n",
    "    # load the model as checkpoint and see multiple information\n",
    "        checkpoint = torch.load(args.name)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        acc = checkpoint['acc']\n",
    "        print('Model trained for', epoch, 'epochs with accuracy', acc, 'and loss', loss)\n",
    "        test(model, args.model, device, test_loader, criterion, input_size, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ab244",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if args.resume:\n",
    "        train_loader, test_loader = get_dataset(args.dataset, args.batch_size) \n",
    "        # load the model as checkpoint and see multiple information\n",
    "        checkpoint = torch.load(args.name)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        acc = checkpoint['acc']\n",
    "        print('Model trained for', epoch, 'epochs with accuracy', acc, 'and loss', loss)\n",
    "        print('Resuming training...')\n",
    "        for epoch in range(epoch+1, epoch + args.epochs):\n",
    "            acc = train_keep_best(epoch, model, args.model, device, train_loader, test_loader, optimizer, criterion, input_size, channels, args.name, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # print the model\n",
    "    if args.print:\n",
    "        print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4363ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # print the model info\n",
    "    if args.info:\n",
    "        print(model)\n",
    "        print('Model Info')\n",
    "        for name, param in model.named_parameters():\n",
    "            print(name, param.shape)\n",
    "        print('Trainable parameters:\\n',sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if args.image:\n",
    "        # classes of cifar10\n",
    "        classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "        # load the model as checkpoint and see multiple information\n",
    "        checkpoint = torch.load(args.name)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        acc = checkpoint['acc']\n",
    "        print('Model trained for', epoch, 'epochs with accuracy', acc, 'and loss', loss)\n",
    "        print('This the image you want to classify:')\n",
    "        # show the image\n",
    "        img = Image.open(args.image)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        # transform the image\n",
    "        transform = transforms.Compose([transforms.Resize((input_size, input_size)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5,), (0.5,))])\n",
    "        \n",
    "        img = transform(img)\n",
    "        # show the transformed image\n",
    "        plt.imshow(img[0], cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "        # add a dimension to the image\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "        # classify the image\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            print('The image is classified as', pred.item(), 'which is', classes[pred.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad3c2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
